{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset (only 10% of rows for testing)\n",
    "file_path = r\"D:\\college\\Sem2\\DataSci ML\\archive\\emails.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check for missing data and duplicates\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "duplicate_count = df.duplicated(subset=['message']).sum()\n",
    "print(f\"Number of duplicate emails: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to extract email components\n",
    "def extract_email_components(email):\n",
    "    from_match = re.search(r'From: (.+)', email)\n",
    "    to_match = re.search(r'To: (.+)', email)\n",
    "    subject_match = re.search(r'Subject: (.*)', email)\n",
    "    body_match = re.search(r'\\n\\n(.*)', email, re.DOTALL)  # Body starts after a blank line\n",
    "    return {\n",
    "        \"From\": from_match.group(1).strip() if from_match else None,\n",
    "        \"To\": to_match.group(1).strip() if to_match else None,\n",
    "        \"Subject\": subject_match.group(1).strip() if subject_match else None,\n",
    "        \"Body\": body_match.group(1).strip() if body_match else None\n",
    "    }\n",
    "\n",
    "# Apply function to all emails\n",
    "email_components = df['message'].apply(extract_email_components)\n",
    "email_df = pd.DataFrame(email_components.tolist())\n",
    "\n",
    "# Merge extracted data with original DataFrame\n",
    "df = pd.concat([df, email_df], axis=1)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values in key fields\n",
    "print(\"Missing values per column:\")\n",
    "print(df[['From', 'To', 'Subject', 'Body']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(re|fw)\\b', '', text)  # Remove \"Re:\" and \"FW:\"\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)   # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to Subject & Body\n",
    "df[\"Processed_Subject\"] = df[\"Subject\"].fillna(\"\").apply(clean_text)\n",
    "df[\"Processed_Body\"] = df[\"Body\"].fillna(\"\").apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(re|fw)\\b', '', text)  # Remove \"Re:\" and \"FW:\"\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)   # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to Subject & Body\n",
    "df[\"Processed_Subject\"] = df[\"Subject\"].fillna(\"\").apply(clean_text)\n",
    "df[\"Processed_Body\"] = df[\"Body\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# Define spam keywords\n",
    "spam_keywords = [\n",
    "    \"win\", \"lottery\", \"free\", \"offer\", \"click here\", \"urgent\", \"claim\", \n",
    "    \"limited-time\", \"money\", \"prize\", \"limited time\"\n",
    "]\n",
    "spam_pattern = re.compile(r'\\b(' + '|'.join(spam_keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Modified function to check if an email is spam based on rule >= 3\n",
    "def is_spam(row):\n",
    "    subject = row[\"Processed_Subject\"] if pd.notnull(row[\"Processed_Subject\"]) else \"\"\n",
    "    body = row[\"Processed_Body\"] if pd.notnull(row[\"Processed_Body\"]) else \"\"\n",
    "    sender = row[\"From\"] if pd.notnull(row[\"From\"]) else \"\"\n",
    "    \n",
    "    score = 0\n",
    "    # Add points for keyword matches\n",
    "    score += 1 if spam_pattern.search(subject) else 0\n",
    "    score += 1 if spam_pattern.search(body) else 0\n",
    "    \n",
    "    # Add points if sender is outside @enron.com domain\n",
    "    score += 2 if sender and not sender.endswith(\"@enron.com\") else 0\n",
    "    \n",
    "    # If score >= 3, classify as spam (1), else not spam (0)\n",
    "    return 1 if score >= 3 else 0\n",
    "\n",
    "# Apply function to label spam\n",
    "df[\"Spam_Label\"] = df.apply(is_spam, axis=1)\n",
    "\n",
    "# Display spam vs non-spam counts\n",
    "print(df[\"Spam_Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack  # Import hstack from scipy.sparse\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split\n",
    "\n",
    "# Drop 'To' column as it's not useful\n",
    "df = df.drop(columns=[\"To\"], errors=\"ignore\")\n",
    "\n",
    "# Remove rows with missing values in key fields\n",
    "df = df.dropna(subset=[\"From\", \"Processed_Subject\", \"Processed_Body\"])\n",
    "\n",
    "# Feature Engineering: Word Count in Subject & Body\n",
    "df[\"Word_Count_Subject\"] = df[\"Processed_Subject\"].apply(lambda x: len(x.split()))\n",
    "df[\"Word_Count_Body\"] = df[\"Processed_Body\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Display updated dataset\n",
    "print(df[[\"Processed_Subject\", \"Processed_Body\", \"Word_Count_Subject\", \"Word_Count_Body\", \"Spam_Label\"]].head())\n",
    "\n",
    "# TF-IDF Vectorization for Subject & Body\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=500, ngram_range=(1, 2))\n",
    "\n",
    "# Transform Subject & Body separately\n",
    "tfidf_subject = vectorizer.fit_transform(df[\"Processed_Subject\"])\n",
    "tfidf_body = vectorizer.fit_transform(df[\"Processed_Body\"])\n",
    "\n",
    "# Merge TF-IDF features into a single sparse matrix\n",
    "X = hstack([tfidf_subject, tfidf_body])  # Sparse format to reduce memory usage\n",
    "y = df[\"Spam_Label\"].values  # Convert labels to NumPy array\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialis and train the k-NN classifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict with test data\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"k-NN Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display Classification Report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=[\"Not Spam\", \"Spam\"], yticklabels=[\"Not Spam\", \"Spam\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - k-NN\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
