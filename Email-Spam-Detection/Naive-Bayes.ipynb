{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Produced by: Kirubel Temesgen\n",
    "# College ID: C00260396\n",
    "# Description: This is a Na誰ve Bayes model implementation for email spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load dataset (only 10% of rows for testing)\n",
    "file_path = r\"D:\\college\\Sem2\\DataSci ML\\archive\\emails.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check for missing data and duplicates\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "duplicate_count = df.duplicated(subset=['message']).sum()\n",
    "print(f\"Number of duplicate emails: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract components from emails\n",
    "def extract_email_parts(email):\n",
    "    from_ = re.search(r'From: (.+)', email)\n",
    "    subject = re.search(r'Subject: (.*)', email)\n",
    "    body = re.search(r'\\n\\n(.*)', email, re.DOTALL)\n",
    "    return {\n",
    "        \"From\": from_.group(1).strip() if from_ else None,\n",
    "        \"Subject\": subject.group(1).strip() if subject else None,\n",
    "        \"Body\": body.group(1).strip() if body else None\n",
    "    }\n",
    "\n",
    "parts = df['message'].apply(extract_email_parts)\n",
    "df = pd.concat([df, pd.DataFrame(parts.tolist())], axis=1)\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna(subset=[\"From\", \"Subject\", \"Body\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(re|fw)\\b', '', text)  # Remove \"Re:\" and \"FW:\"\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)   # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "df[\"Processed_Subject\"] = df[\"Subject\"].apply(clean_text)\n",
    "df[\"Processed_Body\"] = df[\"Body\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spam keywords\n",
    "spam_keywords = [\n",
    "    \"win\", \"lottery\", \"free\", \"offer\", \"click here\", \"urgent\", \"claim\", \n",
    "    \"money\", \"prize\", \"limited time\"\n",
    "]\n",
    "\n",
    "# Compile regex pattern for keyword matching\n",
    "spam_pattern = re.compile(r'\\b(' + '|'.join(spam_keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Function to check if an email is spam\n",
    "# Points based \n",
    "def is_spam(row):\n",
    "    s = row[\"Processed_Subject\"]\n",
    "    b = row[\"Processed_Body\"]\n",
    "    f = row[\"From\"]\n",
    "    score = 0\n",
    "    score += 1 if spam_pattern.search(s) else 0\n",
    "    score += 1 if spam_pattern.search(b) else 0\n",
    "    score += 2 if f and not f.endswith(\"@enron.com\") else 0\n",
    "    return 1 if score >= 3 else 0\n",
    "\n",
    "df[\"Spam_Label\"] = df.apply(is_spam, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Initialise TF-IDF Vectorizer - Text to numerical values\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=500, ngram_range=(1, 2))\n",
    "\n",
    "# Transform Subject & Body separately\n",
    "tfidf_subject = vectorizer.fit_transform(df[\"Processed_Subject\"])\n",
    "tfidf_body = vectorizer.fit_transform(df[\"Processed_Body\"])\n",
    "\n",
    "# Merge TF-IDF features into a single sparse matrix\n",
    "X = hstack([tfidf_subject, tfidf_body])\n",
    "y = df[\"Spam_Label\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Na誰ve Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict using test data\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Na誰ve Bayes Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=[\"Not Spam\", \"Spam\"], yticklabels=[\"Not Spam\", \"Spam\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Na誰ve Bayes\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
